{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e97ccc9",
   "metadata": {},
   "source": [
    "# DigiCow Farmer Training Adoption Challenge\n",
    "## Random Forest with Word2Vec Cluster-Distance Features\n",
    "\n",
    "**Pipeline:** Topic names → Word2Vec → K-Means clustering → Cosine distance features → RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from category_encoders import TargetEncoder\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"All imports successful ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7897a42",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load and combine training data\n",
    "# ============================================================\n",
    "df = pd.read_csv(\"Original Data/Train.csv\")\n",
    "prior_df = pd.read_csv(\"Original Data/Prior.csv\")\n",
    "test_df = pd.read_csv(\"Original Data/Test.csv\")\n",
    "\n",
    "# Parse trainer from list literal\n",
    "df[\"trainer\"] = df[\"trainer\"].apply(lambda x: ast.literal_eval(x)[0])\n",
    "\n",
    "# Combine datasets for Word2Vec training (use all available topic text)\n",
    "combined_df = pd.concat([df, prior_df], ignore_index=True)\n",
    "\n",
    "# Build target columns\n",
    "TARGETS = ['adopted_within_07_days', 'adopted_within_90_days', 'adopted_within_120_days']\n",
    "for t in TARGETS:\n",
    "    df[t] = df[t].astype(int)\n",
    "\n",
    "print(f\"Training data: {df.shape}\")\n",
    "print(f\"Combined data: {combined_df.shape}\")\n",
    "print(f\"Test data:     {test_df.shape}\")\n",
    "\n",
    "for t in TARGETS:\n",
    "    pos_rate = df[t].mean()\n",
    "    print(f\"  {t}: {pos_rate:.4f} ({df[t].sum()}/{len(df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef2f34",
   "metadata": {},
   "source": [
    "## 2. Topic Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "topic_parsing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Topic Parsing & Cleaning\n",
    "# ============================================================\n",
    "def clean_and_flat_topics(raw):\n",
    "    \"\"\"Parse nested topic lists into flat, cleaned list of topic strings.\"\"\"\n",
    "    try:\n",
    "        parsed = ast.literal_eval(raw)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "    flat_topics = []\n",
    "    def flatten(obj):\n",
    "        if isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                flatten(item)\n",
    "        elif isinstance(obj, str):\n",
    "            flat_topics.append(obj)\n",
    "    flatten(parsed)\n",
    "    cleaned = sorted(list(set([t.lower().strip() for t in flat_topics if t])))\n",
    "    return cleaned\n",
    "\n",
    "# Apply to training and combined data\n",
    "df['clean_topics'] = df['topics_list'].apply(clean_and_flat_topics)\n",
    "combined_df['clean_topics'] = combined_df['topics_list'].apply(clean_and_flat_topics)\n",
    "\n",
    "# Collect all unique topics\n",
    "all_topics_flat = []\n",
    "for topics in combined_df['clean_topics']:\n",
    "    all_topics_flat.extend(topics)\n",
    "unique_topics = sorted(set(all_topics_flat))\n",
    "\n",
    "print(f\"Unique topics: {len(unique_topics)}\")\n",
    "print(f\"Total topic mentions: {len(all_topics_flat)}\")\n",
    "print(f\"Avg topics per farmer: {df['clean_topics'].apply(len).mean():.1f}\")\n",
    "print(f\"\\nSample topics:\")\n",
    "for t in unique_topics[:10]:\n",
    "    print(f\"  - {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2b4ef",
   "metadata": {},
   "source": [
    "## 3. Word2Vec Training\n",
    "Train on topic names + farmer topic sets to capture co-occurrence patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "word2vec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Word2Vec Training\n",
    "# ============================================================\n",
    "# Build corpus: each topic name is a \"sentence\" (list of words)\n",
    "# Also include farmer topic sets as sentences for co-occurrence context\n",
    "\n",
    "# 1. Individual topic names as sentences\n",
    "topic_sentences = [t.split() for t in unique_topics]\n",
    "\n",
    "# 2. Farmer topic sets as sentences (captures which topics co-occur)\n",
    "farmer_sentences = []\n",
    "for topics in combined_df['clean_topics']:\n",
    "    if len(topics) > 0:\n",
    "        # Each farmer's topic set as a bag of words\n",
    "        words = []\n",
    "        for t in topics:\n",
    "            words.extend(t.split())\n",
    "        farmer_sentences.append(words)\n",
    "\n",
    "all_sentences = topic_sentences + farmer_sentences\n",
    "print(f\"Word2Vec corpus: {len(all_sentences)} sentences\")\n",
    "print(f\"  Topic name sentences: {len(topic_sentences)}\")\n",
    "print(f\"  Farmer context sentences: {len(farmer_sentences)}\")\n",
    "\n",
    "# Train Word2Vec\n",
    "W2V_DIM = 50  # Embedding dimension (small corpus needs small dim)\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=all_sentences,\n",
    "    vector_size=W2V_DIM,\n",
    "    window=5,\n",
    "    min_count=1,  # Keep all words (small domain vocabulary)\n",
    "    workers=4,\n",
    "    epochs=100,   # More epochs for small corpus\n",
    "    seed=42,\n",
    "    sg=1,         # Skip-gram (better for small datasets)\n",
    ")\n",
    "\n",
    "print(f\"\\nWord2Vec trained:\")\n",
    "print(f\"  Vocabulary size: {len(w2v_model.wv)}\")\n",
    "print(f\"  Embedding dim: {W2V_DIM}\")\n",
    "print(f\"  Epochs: 100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511dbd45",
   "metadata": {},
   "source": [
    "## 4. Topic Embeddings\n",
    "Average word vectors per topic name to get 149 topic embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "topic_embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Topic Embeddings\n",
    "# ============================================================\n",
    "# For each unique topic, average the word vectors to get a topic embedding\n",
    "\n",
    "def get_topic_embedding(topic_name, w2v):\n",
    "    \"\"\"Average word vectors for a topic name.\"\"\"\n",
    "    words = topic_name.split()\n",
    "    vectors = [w2v.wv[w] for w in words if w in w2v.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(w2v.wv.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Build topic embedding matrix\n",
    "topic_embeddings = {}\n",
    "for topic in unique_topics:\n",
    "    topic_embeddings[topic] = get_topic_embedding(topic, w2v_model)\n",
    "\n",
    "topic_names_list = list(topic_embeddings.keys())\n",
    "topic_vectors = np.array([topic_embeddings[t] for t in topic_names_list])\n",
    "\n",
    "print(f\"Topic embedding matrix: {topic_vectors.shape}\")\n",
    "print(f\"  {len(topic_names_list)} topics × {W2V_DIM} dimensions\")\n",
    "\n",
    "# Quick sanity check: find most similar topics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim_matrix = cosine_similarity(topic_vectors)\n",
    "np.fill_diagonal(sim_matrix, 0)\n",
    "\n",
    "print(f\"\\nTop 5 most similar topic pairs:\")\n",
    "for _ in range(5):\n",
    "    i, j = np.unravel_index(sim_matrix.argmax(), sim_matrix.shape)\n",
    "    print(f\"  {sim_matrix[i,j]:.3f}: '{topic_names_list[i]}' ↔ '{topic_names_list[j]}'\")\n",
    "    sim_matrix[i, j] = 0\n",
    "    sim_matrix[j, i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843459a7",
   "metadata": {},
   "source": [
    "## 5. Optimal k Selection\n",
    "Find the best number of clusters using **silhouette scores** (higher = better) and **elbow method** (look for the bend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimal_k",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Optimal k Selection (Silhouette + Elbow)\n",
    "# ============================================================\n",
    "k_range = range(2, 21)\n",
    "silhouette_scores = []\n",
    "inertias = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "    labels = kmeans.fit_predict(topic_vectors)\n",
    "    sil = silhouette_score(topic_vectors, labels)\n",
    "    silhouette_scores.append(sil)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    print(f\"  k={k:2d}: silhouette={sil:.4f}, inertia={kmeans.inertia_:.1f}\")\n",
    "\n",
    "best_k = list(k_range)[np.argmax(silhouette_scores)]\n",
    "best_sil = max(silhouette_scores)\n",
    "print(f\"\\n→ Best k = {best_k} (silhouette = {best_sil:.4f})\")\n",
    "\n",
    "# --- Plot ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Silhouette scores\n",
    "ax1.plot(k_range, silhouette_scores, 'b-o', linewidth=2, markersize=6)\n",
    "ax1.axvline(x=best_k, color='r', linestyle='--', alpha=0.7, label=f'Best k={best_k}')\n",
    "ax1.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax1.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax1.set_title('Silhouette Score vs k', fontsize=14)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Elbow curve\n",
    "ax2.plot(k_range, inertias, 'g-o', linewidth=2, markersize=6)\n",
    "ax2.axvline(x=best_k, color='r', linestyle='--', alpha=0.7, label=f'Best k={best_k}')\n",
    "ax2.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax2.set_ylabel('Inertia (SSE)', fontsize=12)\n",
    "ax2.set_title('Elbow Method', fontsize=14)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Optimal Cluster Selection — Best k = {best_k}', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster_selection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\nPlot saved: cluster_selection.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af3adc",
   "metadata": {},
   "source": [
    "## 6. Clustering & Distance Features\n",
    "Cluster topics, then compute cosine distance from each farmer's average embedding to centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clustering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# K-Means Clustering & Farmer Distance Features\n",
    "# ============================================================\n",
    "BEST_K = best_k\n",
    "print(f\"Clustering with k={BEST_K}\")\n",
    "\n",
    "# Fit final K-Means\n",
    "kmeans_final = KMeans(n_clusters=BEST_K, random_state=42, n_init=10, max_iter=300)\n",
    "topic_cluster_labels = kmeans_final.fit_predict(topic_vectors)\n",
    "centroids = kmeans_final.cluster_centers_\n",
    "\n",
    "# Show cluster contents\n",
    "print(f\"\\nCluster contents:\")\n",
    "for c in range(BEST_K):\n",
    "    members = [topic_names_list[i] for i in range(len(topic_names_list)) if topic_cluster_labels[i] == c]\n",
    "    print(f\"  Cluster {c} ({len(members)} topics): {members[:5]}{'...' if len(members) > 5 else ''}\")\n",
    "\n",
    "# --- Build farmer distance features ---\n",
    "def compute_farmer_cluster_distances(topics_list, topic_emb_dict, centroids, w2v_dim):\n",
    "    \"\"\"For each farmer, average their topic embeddings and compute\n",
    "    cosine distance to each cluster centroid.\n",
    "    Returns k distance features.\n",
    "    \"\"\"\n",
    "    k = len(centroids)\n",
    "    n = len(topics_list)\n",
    "    distances = np.zeros((n, k))\n",
    "\n",
    "    for i, topics in enumerate(topics_list):\n",
    "        # Get embeddings for this farmer's topics\n",
    "        topic_vecs = [topic_emb_dict.get(t, np.zeros(w2v_dim)) for t in topics]\n",
    "        valid_vecs = [v for v in topic_vecs if np.any(v != 0)]\n",
    "\n",
    "        if len(valid_vecs) == 0:\n",
    "            # No valid embeddings — use maximum distance\n",
    "            distances[i, :] = 1.0\n",
    "        else:\n",
    "            farmer_embedding = np.mean(valid_vecs, axis=0)\n",
    "            for c in range(k):\n",
    "                distances[i, c] = cosine(farmer_embedding, centroids[c])\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Compute for training data\n",
    "cluster_distances = compute_farmer_cluster_distances(\n",
    "    df['clean_topics'].values, topic_embeddings, centroids, W2V_DIM\n",
    ")\n",
    "cluster_feature_names = [f'cluster_dist_{c}' for c in range(BEST_K)]\n",
    "cluster_df = pd.DataFrame(cluster_distances, columns=cluster_feature_names, index=df.index)\n",
    "\n",
    "print(f\"\\nCluster distance features: {cluster_df.shape}\")\n",
    "print(f\"  Feature names: {cluster_feature_names}\")\n",
    "print(f\"\\n  Stats:\")\n",
    "print(cluster_df.describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b8c7f",
   "metadata": {},
   "source": [
    "## 7. Feature Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Feature Matrix Assembly\n",
    "# ============================================================\n",
    "# Date features\n",
    "df['training_day'] = pd.to_datetime(df['training_day'], dayfirst=True)\n",
    "df['training_year'] = df['training_day'].dt.year\n",
    "df['training_month'] = df['training_day'].dt.month\n",
    "df['training_day_number'] = df['training_day'].dt.day\n",
    "df['training_dayofweek'] = df['training_day'].dt.dayofweek\n",
    "\n",
    "# Define feature columns\n",
    "CAT_COLS = ['gender', 'registration', 'age', 'group_name', 'county', 'subcounty', 'ward', 'trainer']\n",
    "NUM_COLS = ['belong_to_cooperative', 'has_topic_trained_on',\n",
    "            'training_year', 'training_month', 'training_day_number', 'training_dayofweek']\n",
    "\n",
    "# Assemble\n",
    "X_num = df[NUM_COLS].copy()\n",
    "X_cat = df[CAT_COLS].copy().astype(str).fillna(\"NA\")\n",
    "X_cluster = cluster_df.copy()\n",
    "\n",
    "ALL_FEATURES = NUM_COLS + CAT_COLS + cluster_feature_names\n",
    "print(f\"Feature breakdown:\")\n",
    "print(f\"  Numeric:          {len(NUM_COLS)}\")\n",
    "print(f\"  Categorical:      {len(CAT_COLS)} (target-encoded per fold)\")\n",
    "print(f\"  Cluster distances: {len(cluster_feature_names)}\")\n",
    "print(f\"  Total:            {len(ALL_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54400142",
   "metadata": {},
   "source": [
    "## 8. Random Forest Training\n",
    "5-fold OOF chaining with SMOTE + calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Random Forest with 5-Fold OOF Chaining\n",
    "# ============================================================\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "N_SPLITS = 5\n",
    "SEED = 42\n",
    "\n",
    "target_mapping = {\n",
    "    '7 Days': 'adopted_within_07_days',\n",
    "    '90 Days': 'adopted_within_90_days',\n",
    "    '120 Days': 'adopted_within_120_days',\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "rf_results = {}\n",
    "rf_models = {}\n",
    "rf_target_encoders = {}\n",
    "rf_oof_predictions = {}\n",
    "rf_chained_probs = pd.DataFrame(index=df.index)\n",
    "\n",
    "for period, target in target_mapping.items():\n",
    "    print(f\"\\n{'='*30} {period} {'='*30}\")\n",
    "    y = df[target].values\n",
    "    chain_cols = [c for c in rf_chained_probs.columns]\n",
    "\n",
    "    oof_probs = np.zeros(len(df))\n",
    "    fold_models_list = []\n",
    "    fold_te_list = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(df)), y)):\n",
    "        # Target encode categoricals\n",
    "        te = TargetEncoder(cols=CAT_COLS, smoothing=0.3)\n",
    "        X_train_cat_enc = te.fit_transform(X_cat.iloc[train_idx], y[train_idx])\n",
    "        X_val_cat_enc = te.transform(X_cat.iloc[val_idx])\n",
    "\n",
    "        # Assemble features: numeric + encoded cats + cluster distances + chain\n",
    "        parts_train = [X_num.iloc[train_idx].values, X_train_cat_enc.values, X_cluster.iloc[train_idx].values]\n",
    "        parts_val = [X_num.iloc[val_idx].values, X_val_cat_enc.values, X_cluster.iloc[val_idx].values]\n",
    "        if len(chain_cols) > 0:\n",
    "            parts_train.append(rf_chained_probs[chain_cols].iloc[train_idx].values)\n",
    "            parts_val.append(rf_chained_probs[chain_cols].iloc[val_idx].values)\n",
    "\n",
    "        X_train = np.hstack(parts_train)\n",
    "        X_val = np.hstack(parts_val)\n",
    "\n",
    "        # SMOTE\n",
    "        smote = SMOTE(random_state=SEED, sampling_strategy=0.3)\n",
    "        try:\n",
    "            X_train_sm, y_train_sm = smote.fit_resample(X_train, y[train_idx])\n",
    "        except ValueError:\n",
    "            X_train_sm, y_train_sm = X_train, y[train_idx]\n",
    "\n",
    "        # Train RF + Calibration\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=800, max_features='sqrt', min_samples_leaf=3,\n",
    "            class_weight='balanced_subsample', random_state=SEED, n_jobs=-1,\n",
    "        )\n",
    "        calibrated_rf = CalibratedClassifierCV(estimator=rf, method='sigmoid', cv=3)\n",
    "        calibrated_rf.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "        # OOF predictions\n",
    "        fold_probs = calibrated_rf.predict_proba(X_val)[:, 1]\n",
    "        oof_probs[val_idx] = fold_probs\n",
    "        fold_models_list.append(calibrated_rf)\n",
    "        fold_te_list.append(te)\n",
    "\n",
    "        fold_auc = roc_auc_score(y[val_idx], fold_probs)\n",
    "        print(f\"  Fold {fold+1}: AUC={fold_auc:.4f}\")\n",
    "\n",
    "    # Overall OOF metrics\n",
    "    oof_auc = roc_auc_score(y, oof_probs)\n",
    "    oof_ll = log_loss(y, oof_probs)\n",
    "    print(f\"  ✓ OOF AUC={oof_auc:.4f}, LogLoss={oof_ll:.4f}\")\n",
    "\n",
    "    rf_chained_probs[target] = oof_probs\n",
    "    rf_results[period] = {'auc': oof_auc, 'logloss': oof_ll}\n",
    "    rf_models[period] = fold_models_list\n",
    "    rf_target_encoders[period] = fold_te_list\n",
    "    rf_oof_predictions[period] = oof_probs\n",
    "\n",
    "# --- Summary ---\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"RF RESULTS SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "for period, res in rf_results.items():\n",
    "    print(f\"  {period}: AUC={res['auc']:.4f}, LogLoss={res['logloss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234057d",
   "metadata": {},
   "source": [
    "## 9. Test Inference & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test Inference & Submission\n",
    "# ============================================================\n",
    "# Preprocess test data with same pipeline\n",
    "\n",
    "# Topic parsing\n",
    "test_df['clean_topics'] = test_df['topics_list'].apply(clean_and_flat_topics)\n",
    "\n",
    "# Compute cluster distance features for test data\n",
    "test_cluster_distances = compute_farmer_cluster_distances(\n",
    "    test_df['clean_topics'].values, topic_embeddings, centroids, W2V_DIM\n",
    ")\n",
    "test_cluster_df = pd.DataFrame(test_cluster_distances, columns=cluster_feature_names, index=test_df.index)\n",
    "\n",
    "# Date features\n",
    "test_df['training_day'] = pd.to_datetime(test_df['training_day'], dayfirst=True)\n",
    "test_df['training_year'] = test_df['training_day'].dt.year\n",
    "test_df['training_month'] = test_df['training_day'].dt.month\n",
    "test_df['training_day_number'] = test_df['training_day'].dt.day\n",
    "test_df['training_dayofweek'] = test_df['training_day'].dt.dayofweek\n",
    "\n",
    "X_test_num = test_df[NUM_COLS].copy()\n",
    "X_test_cat = test_df[CAT_COLS].copy().astype(str).fillna(\"NA\")\n",
    "X_test_cluster = test_cluster_df.copy()\n",
    "\n",
    "# Generate submission\n",
    "submission = pd.DataFrame()\n",
    "submission['ID'] = test_df['ID']\n",
    "\n",
    "submission_mapping = {\n",
    "    '7 Days': ['Target_07_AUC', 'Target_07_LogLoss'],\n",
    "    '90 Days': ['Target_90_AUC', 'Target_90_LogLoss'],\n",
    "    '120 Days': ['Target_120_AUC', 'Target_120_LogLoss'],\n",
    "}\n",
    "\n",
    "target_order = [\n",
    "    ('7 Days', 'adopted_within_07_days'),\n",
    "    ('90 Days', 'adopted_within_90_days'),\n",
    "    ('120 Days', 'adopted_within_120_days'),\n",
    "]\n",
    "\n",
    "chained_test_probs = pd.DataFrame(index=test_df.index)\n",
    "\n",
    "for period, target in target_order:\n",
    "    fold_models = rf_models[period]\n",
    "    fold_tes = rf_target_encoders[period]\n",
    "    chain_cols = [c for c in chained_test_probs.columns]\n",
    "    fold_predictions = []\n",
    "\n",
    "    for fold_model, fold_te in zip(fold_models, fold_tes):\n",
    "        X_test_cat_enc = fold_te.transform(X_test_cat)\n",
    "        parts = [X_test_num.values, X_test_cat_enc.values, X_test_cluster.values]\n",
    "        if len(chain_cols) > 0:\n",
    "            parts.append(chained_test_probs[chain_cols].values)\n",
    "        X_test_final = np.hstack(parts)\n",
    "        probs = fold_model.predict_proba(X_test_final)[:, 1]\n",
    "        fold_predictions.append(probs)\n",
    "\n",
    "    avg_probs = np.mean(fold_predictions, axis=0)\n",
    "    for col in submission_mapping[period]:\n",
    "        submission[col] = avg_probs\n",
    "    chained_test_probs[target] = avg_probs\n",
    "    print(f\"  {period}: min={avg_probs.min():.4f}, max={avg_probs.max():.4f}, mean={avg_probs.mean():.4f}\")\n",
    "\n",
    "submission.to_csv('submission_rf_w2v.csv', index=False)\n",
    "print(f\"\\n✓ Saved: submission_rf_w2v.csv\")\n",
    "print(f\"  Shape: {submission.shape}\")\n",
    "print(f\"\\nSubmission preview:\")\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
