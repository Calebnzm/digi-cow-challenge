{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "md_title",
            "metadata": {},
            "source": [
                "# DigiCow Models: RF + NN + RWN\n",
                "\n",
                "**Pipeline features:**\n",
                "- Target Encoding (replaces LabelEncoder)\n",
                "- SMOTE for class imbalance\n",
                "- 5-Fold OOF chaining (fixes data leakage)\n",
                "\n",
                "**Models:**\n",
                "1. Random Forest + CalibratedClassifierCV\n",
                "2. Baseline Neural Network (MLP)\n",
                "3. RWN — Random Forest Weighted Neural Network (Qiu et al., 2024)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "cell_imports",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "All imports loaded. Device: cpu\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package wordnet to /home/nzioka/nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n",
                        "[nltk_data] Downloading package omw-1.4 to /home/nzioka/nltk_data...\n",
                        "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import ast\n",
                "import re\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "import nltk\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "\n",
                "from sklearn.preprocessing import MultiLabelBinarizer\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.calibration import CalibratedClassifierCV\n",
                "from sklearn.metrics import roc_auc_score, log_loss, classification_report, confusion_matrix\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
                "\n",
                "from category_encoders import TargetEncoder\n",
                "from imblearn.over_sampling import SMOTE\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "\n",
                "# Download NLTK data\n",
                "try:\n",
                "    nltk.data.find('corpora/wordnet')\n",
                "except LookupError:\n",
                "    nltk.download('wordnet')\n",
                "    nltk.download('omw-1.4')\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"All imports loaded. Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "md_data",
            "metadata": {},
            "source": [
                "## Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "cell_load_data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Combined dataset: (58418, 17)\n",
                        "\n",
                        "Target distribution:\n",
                        "  adopted_within_07_days: 0.0140 (817/58418)\n",
                        "  adopted_within_90_days: 0.0298 (1742/58418)\n",
                        "  adopted_within_120_days: 0.0409 (2389/58418)\n",
                        "\n",
                        "          ID  farmer_name training_day  gender registration       age  \\\n",
                        "0  ID_CENCC8  FAR_eqbhscj   2024-01-03  Female       Manual  Above 35   \n",
                        "1  ID_YTO0FF  FAR_qlwtyik   2024-01-03  Female       Manual  Above 35   \n",
                        "2  ID_1476PE  FAR_somfzxp   2024-01-03  Female       Manual  Above 35   \n",
                        "3  ID_MLKLIR  FAR_ongcqyd   2024-01-03  Female       Manual  Above 35   \n",
                        "4  ID_V5ZVTA  FAR_ztsbhhm   2024-01-03  Female         Ussd  Below 35   \n",
                        "\n",
                        "    group_name  belong_to_cooperative      county    subcounty          ward  \\\n",
                        "0  GRP_yvpakgc                      0  CNT_lpotuu  SUB_lpotuuf  WRD_lpotuufh   \n",
                        "1  GRP_zemrbsy                      1  CNT_fhdsoy  SUB_mdyljqn  WRD_atkhhvon   \n",
                        "2  GRP_zmblxsw                      0  CNT_fhdsoy  SUB_mdyljqn  WRD_atkhhvon   \n",
                        "3  GRP_psdrfni                      0  CNT_fhdsoy  SUB_mdyljqn  WRD_atkhhvon   \n",
                        "4  GRP_yvpakgc                      0  CNT_lpotuu  SUB_lpotuuf  WRD_lpotuufh   \n",
                        "\n",
                        "   adopted_within_07_days  adopted_within_90_days  adopted_within_120_days  \\\n",
                        "0                       0                       0                        0   \n",
                        "1                       0                       0                        0   \n",
                        "2                       0                       0                        0   \n",
                        "3                       0                       0                        0   \n",
                        "4                       0                       0                        0   \n",
                        "\n",
                        "   has_topic_trained_on       trainer  \\\n",
                        "0                     0  TRA_szrwyfzz   \n",
                        "1                     1  TRA_rkvyofbh   \n",
                        "2                     1  TRA_rkvyofbh   \n",
                        "3                     1  TRA_rkvyofbh   \n",
                        "4                     0  TRA_szrwyfzz   \n",
                        "\n",
                        "                                         topics_list  \n",
                        "0                 [['Ndume App', 'Poultry Feeding']]  \n",
                        "1         [['Poultry Housing'], ['Poultry Housing']]  \n",
                        "2  [['Asili Fertilizer (Organic)', 'Biosecurity I...  \n",
                        "3  [['Poultry Products'], ['Record Keeping In Dai...  \n",
                        "4                 [['Ndume App', 'Poultry Feeding']]  \n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Load and combine training data\n",
                "# ============================================================\n",
                "df = pd.read_csv(\"Original Data/Train.csv\")\n",
                "prior_df = pd.read_csv(\"Original Data/Prior.csv\")\n",
                "\n",
                "# Parse trainer from list literal\n",
                "df[\"trainer\"] = df[\"trainer\"].apply(lambda x: ast.literal_eval(x)[0])\n",
                "\n",
                "# Combine datasets\n",
                "combined_df = pd.concat([df, prior_df], ignore_index=True)\n",
                "df = combined_df.copy()\n",
                "\n",
                "print(f\"Combined dataset: {df.shape}\")\n",
                "print(f\"\\nTarget distribution:\")\n",
                "for t in ['adopted_within_07_days', 'adopted_within_90_days', 'adopted_within_120_days']:\n",
                "    pos_rate = df[t].mean()\n",
                "    print(f\"  {t}: {pos_rate:.4f} ({df[t].sum()}/{len(df)})\")\n",
                "\n",
                "print(f\"\\n{df.head()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "md_preprocess",
            "metadata": {},
            "source": [
                "## Shared Preprocessing\n",
                "Target encoding, TF-IDF, date features. No engineered features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "cell_preprocess",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Created 149 binary topic columns\n",
                        "TF-IDF features: 500\n",
                        "\n",
                        "Feature breakdown:\n",
                        "  Numeric: 6\n",
                        "  Categorical (target-encoded): 8\n",
                        "  TF-IDF: 500\n",
                        "  Total: 514\n",
                        "\n",
                        "Note: Binary topic columns (149) and engineered features are NOT included (ablated).\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Shared Preprocessing\n",
                "# ============================================================\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "# --- Topic Parsing ---\n",
                "def clean_and_flat_topics(topic_str):\n",
                "    if not isinstance(topic_str, str) or pd.isna(topic_str):\n",
                "        return []\n",
                "    try:\n",
                "        parsed = ast.literal_eval(topic_str)\n",
                "    except (ValueError, SyntaxError):\n",
                "        return []\n",
                "    flat_topics = []\n",
                "    def flatten(item):\n",
                "        if isinstance(item, list):\n",
                "            for sub in item:\n",
                "                flatten(sub)\n",
                "        elif isinstance(item, str):\n",
                "            flat_topics.append(item)\n",
                "    flatten(parsed)\n",
                "    cleaned = sorted(list(set([t.lower().strip() for t in flat_topics if t])))\n",
                "    return cleaned\n",
                "\n",
                "df['clean_topics'] = df['topics_list'].apply(clean_and_flat_topics)\n",
                "\n",
                "# MultiLabelBinarizer for topic columns (used for TF-IDF text generation)\n",
                "mlb = MultiLabelBinarizer()\n",
                "topics_encoded = mlb.fit_transform(df['clean_topics'])\n",
                "topic_columns = [f'topic_{t}' for t in mlb.classes_]\n",
                "topics_df = pd.DataFrame(topics_encoded, columns=topic_columns, index=df.index)\n",
                "df = pd.concat([df, topics_df], axis=1)\n",
                "\n",
                "print(f\"Created {len(topic_columns)} binary topic columns\")\n",
                "\n",
                "# --- Topic text for TF-IDF ---\n",
                "def lemmatize_text(text):\n",
                "    words = text.split()\n",
                "    lemmatized = []\n",
                "    for word in words:\n",
                "        lemma = lemmatizer.lemmatize(word, pos='n')\n",
                "        if lemma == word:\n",
                "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
                "        lemmatized.append(lemma)\n",
                "    return ' '.join(lemmatized)\n",
                "\n",
                "def topics_to_text(row):\n",
                "    active_topics = [col.replace('topic_', '') for col in topic_columns if row[col] == 1]\n",
                "    if not active_topics:\n",
                "        return 'no_topics'\n",
                "    text = ' '.join(active_topics)\n",
                "    text = text.lower().replace('(', '').replace(')', '').replace('-', ' ').replace('_', ' ')\n",
                "    text = lemmatize_text(text)\n",
                "    return text\n",
                "\n",
                "df['topic_text'] = df[topic_columns].apply(topics_to_text, axis=1)\n",
                "\n",
                "# --- Date Features ---\n",
                "df['training_day'] = pd.to_datetime(df['training_day'], dayfirst=True)\n",
                "df['training_year'] = df['training_day'].dt.year\n",
                "df['training_month'] = df['training_day'].dt.month\n",
                "df['training_day_number'] = df['training_day'].dt.day\n",
                "df['training_dayofweek'] = df['training_day'].dt.dayofweek\n",
                "\n",
                "# --- Define targets and feature columns ---\n",
                "TARGETS = ['adopted_within_07_days', 'adopted_within_90_days', 'adopted_within_120_days']\n",
                "\n",
                "# Categorical columns for target encoding\n",
                "CAT_COLS = ['gender', 'registration', 'age', 'group_name', 'county', 'subcounty', 'ward', 'trainer']\n",
                "\n",
                "# Numeric base features\n",
                "NUM_COLS = ['belong_to_cooperative', 'has_topic_trained_on',\n",
                "            'training_year', 'training_month', 'training_day_number', 'training_dayofweek']\n",
                "\n",
                "# --- TF-IDF ---\n",
                "custom_stopwords = list(set(ENGLISH_STOP_WORDS).union({\n",
                "    'how', 'to', 'from', 'with', 'your', 'for', 'the', 'and', 'in', 'of', 'a', 'an',\n",
                "    'day', 'old', 'care', 'using', 'about', 'on', 'at', 'by', 'after', 'before',\n",
                "    'week', 'weeks', 'maturity', 'products', 'product', 'use', 'uses', 'used',\n",
                "    'new', 'best', 'good', 'better', 'right', 'proper', 'important', 'importance'\n",
                "}))\n",
                "\n",
                "tfidf = TfidfVectorizer(\n",
                "    max_features=500,\n",
                "    ngram_range=(1, 2),\n",
                "    min_df=3,\n",
                "    max_df=0.85,\n",
                "    sublinear_tf=True,\n",
                "    stop_words=custom_stopwords,\n",
                "    lowercase=True,\n",
                "    token_pattern=r'\\b[a-z]{3,}\\b'\n",
                ")\n",
                "\n",
                "tfidf_matrix = tfidf.fit_transform(df['topic_text'])\n",
                "tfidf_feature_names = [f'tfidf_{f}' for f in tfidf.get_feature_names_out()]\n",
                "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_feature_names, index=df.index)\n",
                "\n",
                "print(f\"TF-IDF features: {len(tfidf_feature_names)}\")\n",
                "\n",
                "# --- Assemble feature matrix ---\n",
                "# Base numeric features\n",
                "X_num = df[NUM_COLS].copy()\n",
                "\n",
                "# Categorical features (will be target-encoded per fold)\n",
                "X_cat = df[CAT_COLS].copy().astype(str).fillna(\"NA\")\n",
                "\n",
                "# TF-IDF features\n",
                "X_tfidf = tfidf_df.copy()\n",
                "\n",
                "# Full feature names (for reference)\n",
                "ALL_FEATURES = NUM_COLS + CAT_COLS + tfidf_feature_names\n",
                "\n",
                "print(f\"\\nFeature breakdown:\")\n",
                "print(f\"  Numeric: {len(NUM_COLS)}\")\n",
                "print(f\"  Categorical (target-encoded): {len(CAT_COLS)}\")\n",
                "print(f\"  TF-IDF: {len(tfidf_feature_names)}\")\n",
                "print(f\"  Total: {len(ALL_FEATURES)}\")\n",
                "print(f\"\\nNote: Binary topic columns ({len(topic_columns)}) and engineered features are NOT included (ablated).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "md_ablation",
            "metadata": {},
            "source": [
                "## Feature Ablation Study\n",
                "Quick test confirms 'Base + TF-IDF' works better than adding engineered features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "cell_ablation",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Building engineered features for ablation test...\n",
                        "Engineered features: 26\n",
                        "  Base + TF-IDF ONLY: AUC=0.9735, LogLoss=0.1120\n",
                        "  Base + TF-IDF + Engineered: AUC=0.9726, LogLoss=0.1067\n",
                        "\n",
                        "→ If 'Base + TF-IDF ONLY' is similar or better, engineered features are noise.\n",
                        "  We proceed WITHOUT engineered features for cleaner models.\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Feature Ablation Study\n",
                "# ============================================================\n",
                "# Compare: (A) base + TF-IDF only  vs  (B) base + TF-IDF + engineered features\n",
                "# This confirms whether removing engineered features is the right call.\n",
                "\n",
                "# --- Build engineered features for ablation test ---\n",
                "def build_engineered_features(df_in, topic_cols):\n",
                "    \"\"\"Build the 26 engineered domain features from binary topic columns.\"\"\"\n",
                "    eng = pd.DataFrame(index=df_in.index)\n",
                "\n",
                "    livestock_kw = ['poultry', 'dairy', 'cow', 'calf', 'chicken', 'kienyeji', 'layer', 'breed', 'milking', 'herd']\n",
                "    crop_kw = ['maize', 'bean', 'seed', 'weed', 'fertilizer', 'crop', 'pest']\n",
                "    business_kw = ['record', 'ndume', 'app', 'market', 'product', 'management', 'keeping']\n",
                "    health_kw = ['health', 'disease', 'vaccination', 'biosecurity', 'hygiene', 'deworming', 'antimicrobial']\n",
                "    feed_kw = ['feed', 'feeding', 'nutrition', 'mineral', 'supplementation', 'tyari']\n",
                "\n",
                "    def count_domain(row, keywords):\n",
                "        return sum(1 for col in topic_cols if row[col] == 1 and any(k in col.lower() for k in keywords))\n",
                "\n",
                "    eng['topic_count'] = df_in[topic_cols].sum(axis=1)\n",
                "    eng['topic_diversity'] = eng['topic_count'] / max(len(topic_cols), 1)\n",
                "    eng['livestock_count'] = df_in.apply(lambda r: count_domain(r, livestock_kw), axis=1)\n",
                "    eng['crop_count'] = df_in.apply(lambda r: count_domain(r, crop_kw), axis=1)\n",
                "    eng['business_count'] = df_in.apply(lambda r: count_domain(r, business_kw), axis=1)\n",
                "    eng['health_count'] = df_in.apply(lambda r: count_domain(r, health_kw), axis=1)\n",
                "    eng['feed_count'] = df_in.apply(lambda r: count_domain(r, feed_kw), axis=1)\n",
                "\n",
                "    for domain in ['livestock', 'crop', 'business', 'health', 'feed']:\n",
                "        eng[f'{domain}_pct'] = eng[f'{domain}_count'] / eng['topic_count'].clip(lower=1)\n",
                "\n",
                "    for domain in ['livestock', 'crop', 'business', 'health', 'feed']:\n",
                "        eng[f'has_{domain}'] = (eng[f'{domain}_count'] > 0).astype(int)\n",
                "\n",
                "    eng['has_livestock_and_business'] = ((eng['livestock_count'] > 0) & (eng['business_count'] > 0)).astype(int)\n",
                "    eng['has_livestock_and_health'] = ((eng['livestock_count'] > 0) & (eng['health_count'] > 0)).astype(int)\n",
                "    eng['has_crop_and_business'] = ((eng['crop_count'] > 0) & (eng['business_count'] > 0)).astype(int)\n",
                "\n",
                "    domain_cols = ['livestock_count', 'crop_count', 'business_count', 'health_count', 'feed_count']\n",
                "    eng['num_domains_covered'] = (eng[domain_cols] > 0).sum(axis=1)\n",
                "    eng['is_specialist'] = (eng['num_domains_covered'] == 1).astype(int)\n",
                "    eng['is_diversified'] = (eng['num_domains_covered'] >= 4).astype(int)\n",
                "    eng['has_comprehensive_training'] = (eng['num_domains_covered'] >= 3).astype(int)\n",
                "    eng['max_domain_count'] = eng[domain_cols].max(axis=1)\n",
                "    eng['avg_topics_per_domain'] = eng['topic_count'] / eng['num_domains_covered'].clip(lower=1)\n",
                "\n",
                "    return eng\n",
                "\n",
                "print(\"Building engineered features for ablation test...\")\n",
                "eng_features_df = build_engineered_features(df, topic_columns)\n",
                "ENG_FEATURE_NAMES = eng_features_df.columns.tolist()\n",
                "print(f\"Engineered features: {len(ENG_FEATURE_NAMES)}\")\n",
                "\n",
                "# --- Quick ablation with RF on the first target (7-day) ---\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from category_encoders import TargetEncoder\n",
                "\n",
                "target = 'adopted_within_07_days'\n",
                "y_abl = df[target].values\n",
                "skf_abl = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
                "\n",
                "for ablation_name, extra_df in [(\"Base + TF-IDF ONLY\", None), (\"Base + TF-IDF + Engineered\", eng_features_df)]:\n",
                "    oof_probs = np.zeros(len(df))\n",
                "\n",
                "    for fold, (train_idx, val_idx) in enumerate(skf_abl.split(np.zeros(len(df)), y_abl)):\n",
                "        # Build X\n",
                "        X_train_num = X_num.iloc[train_idx].copy()\n",
                "        X_val_num = X_num.iloc[val_idx].copy()\n",
                "\n",
                "        # Target encode categoricals\n",
                "        te = TargetEncoder(cols=CAT_COLS, smoothing=0.3)\n",
                "        X_train_cat_enc = te.fit_transform(X_cat.iloc[train_idx], y_abl[train_idx])\n",
                "        X_val_cat_enc = te.transform(X_cat.iloc[val_idx])\n",
                "\n",
                "        X_train_tfidf = X_tfidf.iloc[train_idx].values\n",
                "        X_val_tfidf = X_tfidf.iloc[val_idx].values\n",
                "\n",
                "        X_tr = np.hstack([X_train_num.values, X_train_cat_enc.values, X_train_tfidf])\n",
                "        X_va = np.hstack([X_val_num.values, X_val_cat_enc.values, X_val_tfidf])\n",
                "\n",
                "        if extra_df is not None:\n",
                "            X_tr = np.hstack([X_tr, extra_df.iloc[train_idx].values])\n",
                "            X_va = np.hstack([X_va, extra_df.iloc[val_idx].values])\n",
                "\n",
                "        rf = RandomForestClassifier(\n",
                "            n_estimators=300, max_features=\"sqrt\", min_samples_leaf=3,\n",
                "            class_weight=\"balanced_subsample\", random_state=42, n_jobs=-1\n",
                "        )\n",
                "        rf.fit(X_tr, y_abl[train_idx])\n",
                "        oof_probs[val_idx] = rf.predict_proba(X_va)[:, 1]\n",
                "\n",
                "    auc = roc_auc_score(y_abl, oof_probs)\n",
                "    ll = log_loss(y_abl, oof_probs)\n",
                "    print(f\"  {ablation_name}: AUC={auc:.4f}, LogLoss={ll:.4f}\")\n",
                "\n",
                "print(\"\\n→ If 'Base + TF-IDF ONLY' is similar or better, engineered features are noise.\")\n",
                "print(\"  We proceed WITHOUT engineered features for cleaner models.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "md_rf",
            "metadata": {},
            "source": [
                "## Random Forest (5-Fold OOF + SMOTE + Calibration)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "cell_rf",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "======================================================================\n",
                        "RANDOM FOREST MODEL\n",
                        "======================================================================\n",
                        "\n",
                        "============================== 7 Days ==============================\n",
                        "  Fold 1: AUC=0.9710\n",
                        "  Fold 2: AUC=0.9741\n",
                        "  Fold 3: AUC=0.9687\n",
                        "  Fold 4: AUC=0.9745\n",
                        "  Fold 5: AUC=0.9761\n",
                        "  ✓ OOF AUC=0.9728, LogLoss=0.0602\n",
                        "\n",
                        "============================== 90 Days ==============================\n",
                        "  Fold 1: AUC=0.9512\n",
                        "  Fold 2: AUC=0.9493\n",
                        "  Fold 3: AUC=0.9551\n",
                        "  Fold 4: AUC=0.9587\n",
                        "  Fold 5: AUC=0.9571\n",
                        "  ✓ OOF AUC=0.9545, LogLoss=0.0912\n",
                        "\n",
                        "============================== 120 Days ==============================\n",
                        "  Fold 1: AUC=0.9417\n",
                        "  Fold 2: AUC=0.9398\n",
                        "  Fold 3: AUC=0.9476\n",
                        "  Fold 4: AUC=0.9405\n",
                        "  Fold 5: AUC=0.9442\n",
                        "  ✓ OOF AUC=0.9426, LogLoss=0.1147\n",
                        "\n",
                        "======================================================================\n",
                        "RF SUMMARY:\n",
                        "  7 Days: AUC=0.9728, LogLoss=0.0602\n",
                        "  90 Days: AUC=0.9545, LogLoss=0.0912\n",
                        "  120 Days: AUC=0.9426, LogLoss=0.1147\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Random Forest — 5-Fold OOF + SMOTE + Calibration\n",
                "# ============================================================\n",
                "print(\"=\" * 70)\n",
                "print(\"RANDOM FOREST MODEL\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "target_mapping = {\n",
                "    '7 Days': 'adopted_within_07_days',\n",
                "    '90 Days': 'adopted_within_90_days',\n",
                "    '120 Days': 'adopted_within_120_days',\n",
                "}\n",
                "\n",
                "N_SPLITS = 5\n",
                "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
                "\n",
                "rf_results = {}\n",
                "rf_models = {}  # Store fold models for inference\n",
                "rf_target_encoders = {}  # Store target encoders per fold\n",
                "rf_oof_predictions = {}  # OOF predictions for chaining\n",
                "\n",
                "# We'll accumulate chained features\n",
                "rf_chained_probs = pd.DataFrame(index=df.index)\n",
                "\n",
                "for period, target in target_mapping.items():\n",
                "    print(f\"\\n{'='*30} {period} {'='*30}\")\n",
                "\n",
                "    y = df[target].values\n",
                "\n",
                "    # Collect features: numeric + categorical + tfidf + any previous OOF chains\n",
                "    chain_cols = [c for c in rf_chained_probs.columns]\n",
                "\n",
                "    oof_probs = np.zeros(len(df))\n",
                "    fold_models_list = []\n",
                "    fold_te_list = []\n",
                "\n",
                "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(df)), y)):\n",
                "        # --- Target encode categoricals ---\n",
                "        te = TargetEncoder(cols=CAT_COLS, smoothing=0.3)\n",
                "        X_train_cat_enc = te.fit_transform(X_cat.iloc[train_idx], y[train_idx])\n",
                "        X_val_cat_enc = te.transform(X_cat.iloc[val_idx])\n",
                "\n",
                "        # --- Assemble features ---\n",
                "        parts_train = [X_num.iloc[train_idx].values, X_train_cat_enc.values, X_tfidf.iloc[train_idx].values]\n",
                "        parts_val = [X_num.iloc[val_idx].values, X_val_cat_enc.values, X_tfidf.iloc[val_idx].values]\n",
                "\n",
                "        # Add chained OOF predictions from previous targets\n",
                "        if len(chain_cols) > 0:\n",
                "            parts_train.append(rf_chained_probs[chain_cols].iloc[train_idx].values)\n",
                "            parts_val.append(rf_chained_probs[chain_cols].iloc[val_idx].values)\n",
                "\n",
                "        X_train = np.hstack(parts_train)\n",
                "        X_val = np.hstack(parts_val)\n",
                "\n",
                "        # --- SMOTE on training data ---\n",
                "        smote = SMOTE(random_state=42, sampling_strategy=0.3)\n",
                "        try:\n",
                "            X_train_sm, y_train_sm = smote.fit_resample(X_train, y[train_idx])\n",
                "        except ValueError:\n",
                "            X_train_sm, y_train_sm = X_train, y[train_idx]\n",
                "\n",
                "        # --- Train RF + Calibration ---\n",
                "        # High performance config (from v1)\n",
                "        rf = RandomForestClassifier(\n",
                "            n_estimators=800,\n",
                "            max_features=\"sqrt\",\n",
                "            min_samples_leaf=3,\n",
                "            class_weight=\"balanced_subsample\",\n",
                "            random_state=42,\n",
                "            n_jobs=-1\n",
                "        )\n",
                "        calibrated_rf = CalibratedClassifierCV(estimator=rf, method=\"sigmoid\", cv=3)\n",
                "        calibrated_rf.fit(X_train_sm, y_train_sm)\n",
                "\n",
                "        # --- OOF predictions ---\n",
                "        fold_probs = calibrated_rf.predict_proba(X_val)[:, 1]\n",
                "        oof_probs[val_idx] = fold_probs\n",
                "\n",
                "        fold_models_list.append(calibrated_rf)\n",
                "        fold_te_list.append(te)\n",
                "\n",
                "        fold_auc = roc_auc_score(y[val_idx], fold_probs)\n",
                "        print(f\"  Fold {fold+1}: AUC={fold_auc:.4f}\")\n",
                "\n",
                "    # --- Overall OOF metrics ---\n",
                "    oof_auc = roc_auc_score(y, oof_probs)\n",
                "    oof_ll = log_loss(y, oof_probs)\n",
                "    print(f\"  ✓ OOF AUC={oof_auc:.4f}, LogLoss={oof_ll:.4f}\")\n",
                "\n",
                "    # Store OOF predictions for chaining to next target\n",
                "    rf_chained_probs[target] = oof_probs\n",
                "\n",
                "    rf_results[period] = {'auc': oof_auc, 'logloss': oof_ll}\n",
                "    rf_models[period] = fold_models_list\n",
                "    rf_target_encoders[period] = fold_te_list\n",
                "    rf_oof_predictions[period] = oof_probs\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"RF SUMMARY:\")\n",
                "for period, res in rf_results.items():\n",
                "    print(f\"  {period}: AUC={res['auc']:.4f}, LogLoss={res['logloss']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "md_nn",
            "metadata": {},
            "source": [
                "## Baseline Neural Network (5-Fold OOF + SMOTE)\n",
                "PyTorch MLP: 512→256→128→256→1 (from RWN paper architecture)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_nn",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "======================================================================\n",
                        "BASELINE NEURAL NETWORK\n",
                        "======================================================================\n",
                        "\n",
                        "============================== 7 Days ==============================\n",
                        "  Fold 1: AUC=0.8833, val_loss=0.0838\n",
                        "  Fold 2: AUC=0.9429, val_loss=0.0540\n",
                        "  Fold 3: AUC=0.7949, val_loss=0.1007\n",
                        "  Fold 4: AUC=0.9360, val_loss=0.0698\n",
                        "  Fold 5: AUC=0.8918, val_loss=0.0729\n",
                        "  ✓ OOF AUC=0.8765, LogLoss=0.0762\n",
                        "\n",
                        "============================== 90 Days ==============================\n",
                        "  Fold 1: AUC=0.8582, val_loss=0.1450\n",
                        "  Fold 2: AUC=0.6554, val_loss=0.1497\n",
                        "  Fold 3: AUC=0.9081, val_loss=0.1292\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Baseline Neural Network — 5-Fold OOF + SMOTE\n",
                "# ============================================================\n",
                "# Architecture from the RWN paper (Qiu et al., 2024):\n",
                "#   h -> h/2 -> h/4 -> h/2 -> 1  (h=512)\n",
                "# ReLU activations, sigmoid output, BCE loss, Adam optimizer\n",
                "# ============================================================\n",
                "print(\"=\" * 70)\n",
                "print(\"BASELINE NEURAL NETWORK\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "class DigiCowMLP(nn.Module):\n",
                "    \"\"\"MLP with architecture h -> h/2 -> h/4 -> h/2 -> 1.\"\"\"\n",
                "    def __init__(self, input_dim, h=512, dropout=0.3):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(input_dim, h),\n",
                "            nn.ReLU(),\n",
                "            nn.BatchNorm1d(h),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(h, h // 2),\n",
                "            nn.ReLU(),\n",
                "            nn.BatchNorm1d(h // 2),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(h // 2, h // 4),\n",
                "            nn.ReLU(),\n",
                "            nn.BatchNorm1d(h // 4),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(h // 4, h // 2),\n",
                "            nn.ReLU(),\n",
                "            nn.BatchNorm1d(h // 2),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(h // 2, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "def train_nn_model(X_train, y_train, X_val, y_val, input_dim,\n",
                "                   h=512, lr=1e-3, batch_size=100, max_epochs=2000,\n",
                "                   patience=50, dropout=0.3):\n",
                "    \"\"\"Train the baseline MLP with early stopping.\"\"\"\n",
                "    model = DigiCowMLP(input_dim, h=h, dropout=dropout).to(device)\n",
                "    criterion = nn.BCEWithLogitsLoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
                "\n",
                "    X_tr = torch.FloatTensor(X_train).to(device)\n",
                "    y_tr = torch.FloatTensor(y_train).to(device)\n",
                "    X_va = torch.FloatTensor(X_val).to(device)\n",
                "    y_va = torch.FloatTensor(y_val).to(device)\n",
                "\n",
                "    dataset = TensorDataset(X_tr, y_tr)\n",
                "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
                "\n",
                "    best_val_loss = float('inf')\n",
                "    best_state = None\n",
                "    wait = 0\n",
                "\n",
                "    for epoch in range(max_epochs):\n",
                "        model.train()\n",
                "        for xb, yb in loader:\n",
                "            optimizer.zero_grad()\n",
                "            logits = model(xb).squeeze()\n",
                "            loss = criterion(logits, yb)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "\n",
                "        # Validation\n",
                "        model.eval()\n",
                "        with torch.no_grad():\n",
                "            val_logits = model(X_va).squeeze()\n",
                "            val_loss = criterion(val_logits, y_va).item()\n",
                "\n",
                "        if val_loss < best_val_loss - 1e-5:\n",
                "            best_val_loss = val_loss\n",
                "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
                "            wait = 0\n",
                "        else:\n",
                "            wait += 1\n",
                "            if wait >= patience:\n",
                "                break\n",
                "\n",
                "    model.load_state_dict(best_state)\n",
                "    model.eval()\n",
                "    return model, best_val_loss\n",
                "\n",
                "# --- 5-Fold OOF Training ---\n",
                "nn_results = {}\n",
                "nn_models = {}\n",
                "nn_target_encoders = {}\n",
                "nn_oof_predictions = {}\n",
                "nn_chained_probs = pd.DataFrame(index=df.index)\n",
                "\n",
                "for period, target in target_mapping.items():\n",
                "    print(f\"\\n{'='*30} {period} {'='*30}\")\n",
                "    y = df[target].values\n",
                "    chain_cols = [c for c in nn_chained_probs.columns]\n",
                "\n",
                "    oof_probs = np.zeros(len(df))\n",
                "    fold_models_list = []\n",
                "    fold_te_list = []\n",
                "\n",
                "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(df)), y)):\n",
                "        te = TargetEncoder(cols=CAT_COLS, smoothing=0.3)\n",
                "        X_train_cat_enc = te.fit_transform(X_cat.iloc[train_idx], y[train_idx])\n",
                "        X_val_cat_enc = te.transform(X_cat.iloc[val_idx])\n",
                "\n",
                "        parts_train = [X_num.iloc[train_idx].values, X_train_cat_enc.values, X_tfidf.iloc[train_idx].values]\n",
                "        parts_val = [X_num.iloc[val_idx].values, X_val_cat_enc.values, X_tfidf.iloc[val_idx].values]\n",
                "        if len(chain_cols) > 0:\n",
                "            parts_train.append(nn_chained_probs[chain_cols].iloc[train_idx].values)\n",
                "            parts_val.append(nn_chained_probs[chain_cols].iloc[val_idx].values)\n",
                "\n",
                "        X_train = np.hstack(parts_train)\n",
                "        X_val = np.hstack(parts_val)\n",
                "\n",
                "        smote = SMOTE(random_state=42, sampling_strategy=0.3)\n",
                "        try:\n",
                "            X_train_sm, y_train_sm = smote.fit_resample(X_train, y[train_idx])\n",
                "        except ValueError:\n",
                "            X_train_sm, y_train_sm = X_train, y[train_idx]\n",
                "\n",
                "        input_dim = X_train_sm.shape[1]\n",
                "        model, val_loss = train_nn_model(\n",
                "            X_train_sm, y_train_sm, X_val, y[val_idx],\n",
                "            input_dim=input_dim, h=512, lr=1e-3, batch_size=100,\n",
                "            max_epochs=2000, patience=50, dropout=0.3\n",
                "        )\n",
                "\n",
                "        with torch.no_grad():\n",
                "            X_va_t = torch.FloatTensor(X_val).to(device)\n",
                "            logits = model(X_va_t).squeeze()\n",
                "            fold_probs = torch.sigmoid(logits).cpu().numpy()\n",
                "\n",
                "        oof_probs[val_idx] = fold_probs\n",
                "        fold_models_list.append(model)\n",
                "        fold_te_list.append(te)\n",
                "\n",
                "        fold_auc = roc_auc_score(y[val_idx], fold_probs)\n",
                "        print(f\"  Fold {fold+1}: AUC={fold_auc:.4f}, val_loss={val_loss:.4f}\")\n",
                "\n",
                "    oof_auc = roc_auc_score(y, oof_probs)\n",
                "    oof_ll = log_loss(y, oof_probs)\n",
                "    print(f\"  ✓ OOF AUC={oof_auc:.4f}, LogLoss={oof_ll:.4f}\")\n",
                "\n",
                "    nn_chained_probs[target] = oof_probs\n",
                "    nn_results[period] = {'auc': oof_auc, 'logloss': oof_ll}\n",
                "    nn_models[period] = fold_models_list\n",
                "    nn_target_encoders[period] = fold_te_list\n",
                "    nn_oof_predictions[period] = oof_probs\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"NEURAL NETWORK SUMMARY:\")\n",
                "for period, res in nn_results.items():\n",
                "    print(f\"  {period}: AUC={res['auc']:.4f}, LogLoss={res['logloss']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "md_rwn",
            "metadata": {},
            "source": [
                "## RWN — Random Forest Weighted Neural Network\n",
                "From Qiu et al. (2024, JRSS-B). Uses RF kernel weights + NN with combined loss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_rwn",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "======================================================================\n",
                        "RWN — RANDOM FOREST WEIGHTED NEURAL NETWORK\n",
                        "======================================================================\n",
                        "\n",
                        "Selecting τ via 3-fold CV on 7-day target...\n",
                        "  Training RF for kernel weights...\n",
                        "  Computing RF kernel weights...\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# RWN — Random Forest Weighted Neural Network\n",
                "# ============================================================\n",
                "# From: Qiu et al. (2024), JRSS-B\n",
                "# Loss = τ × BCE(f(X_i), Y_i) + (1-τ) × Σ_{i≠j} BCE(f(X_i), Y_j) × w(X_i, X_j)\n",
                "# w(x, x') = fraction of RF trees where x and x' share a leaf\n",
                "# ============================================================\n",
                "print(\"=\" * 70)\n",
                "print(\"RWN — RANDOM FOREST WEIGHTED NEURAL NETWORK\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "def compute_rf_kernel_weights(rf_model, X, max_pairs=50000):\n",
                "    \"\"\"Compute RF kernel weights using leaf co-occurrence.\n",
                "    Returns sparse weight info: (i, j, w_ij) for pairs that share leaves.\n",
                "    Uses rf.apply() to get leaf indices, then computes co-occurrence.\n",
                "    \"\"\"\n",
                "    # Get leaf indices for all samples across all trees\n",
                "    leaf_indices = rf_model.apply(X)  # shape: (n_samples, n_trees)\n",
                "    n_samples, n_trees = leaf_indices.shape\n",
                "\n",
                "    # Build co-occurrence counts efficiently\n",
                "    # For each tree, group samples by leaf and count shared pairs\n",
                "    from collections import defaultdict\n",
                "    pair_counts = defaultdict(int)\n",
                "\n",
                "    for t in range(n_trees):\n",
                "        # Group samples by their leaf in this tree\n",
                "        leaf_groups = defaultdict(list)\n",
                "        for i in range(n_samples):\n",
                "            leaf_groups[leaf_indices[i, t]].append(i)\n",
                "\n",
                "        # All pairs within each leaf share this tree\n",
                "        for leaf, members in leaf_groups.items():\n",
                "            if len(members) > 1:\n",
                "                for ii in range(len(members)):\n",
                "                    for jj in range(ii + 1, len(members)):\n",
                "                        key = (members[ii], members[jj])\n",
                "                        pair_counts[key] += 1\n",
                "\n",
                "    # Normalize by number of trees to get w(x_i, x_j)\n",
                "    weights = {}\n",
                "    for (i, j), count in pair_counts.items():\n",
                "        w = count / n_trees\n",
                "        if w > 0.01:  # Only keep meaningful weights\n",
                "            weights[(i, j)] = w\n",
                "            weights[(j, i)] = w\n",
                "\n",
                "    print(f\"    RF kernel: {len(weights)//2} unique pairs with w > 0.01\")\n",
                "    return weights\n",
                "\n",
                "def train_rwn_model(X_train, y_train, X_val, y_val, input_dim,\n",
                "                    rf_weights, train_indices,\n",
                "                    tau=None, h=512, lr=1e-3, batch_size=100,\n",
                "                    max_epochs=2000, patience=50, dropout=0.3):\n",
                "    \"\"\"Train RWN with combined global + local RF-weighted loss.\"\"\"\n",
                "    n = len(X_train)\n",
                "    if tau is None:\n",
                "        tau = 1.0 / n  # Paper default: τ = 1/n\n",
                "\n",
                "    model = DigiCowMLP(input_dim, h=h, dropout=dropout).to(device)\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
                "\n",
                "    X_tr = torch.FloatTensor(X_train).to(device)\n",
                "    y_tr = torch.FloatTensor(y_train).to(device)\n",
                "    X_va = torch.FloatTensor(X_val).to(device)\n",
                "    y_va = torch.FloatTensor(y_val).to(device)\n",
                "\n",
                "    # Pre-compute local weight pairs for training set\n",
                "    # Map global indices to local (training set) indices\n",
                "    global_to_local = {g: l for l, g in enumerate(train_indices)}\n",
                "\n",
                "    # Build sparse local weight tensor\n",
                "    local_pairs_i = []\n",
                "    local_pairs_j = []\n",
                "    local_weights = []\n",
                "    for (gi, gj), w in rf_weights.items():\n",
                "        if gi in global_to_local and gj in global_to_local:\n",
                "            li, lj = global_to_local[gi], global_to_local[gj]\n",
                "            local_pairs_i.append(li)\n",
                "            local_pairs_j.append(lj)\n",
                "            local_weights.append(w)\n",
                "\n",
                "    if len(local_weights) == 0:\n",
                "        print(\"    Warning: No RF weight pairs found, falling back to standard NN\")\n",
                "        tau = 1.0  # No local term\n",
                "\n",
                "    pairs_i_t = torch.LongTensor(local_pairs_i).to(device)\n",
                "    pairs_j_t = torch.LongTensor(local_pairs_j).to(device)\n",
                "    weights_t = torch.FloatTensor(local_weights).to(device)\n",
                "\n",
                "    bce_element = nn.BCEWithLogitsLoss(reduction='mean')\n",
                "    best_val_loss = float('inf')\n",
                "    best_state = None\n",
                "    wait = 0\n",
                "\n",
                "    for epoch in range(max_epochs):\n",
                "        model.train()\n",
                "\n",
                "        # Sample a batch for global loss\n",
                "        perm = torch.randperm(n)[:batch_size]\n",
                "        xb = X_tr[perm]\n",
                "        yb = y_tr[perm]\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        # --- Global loss: standard BCE ---\n",
                "        logits_global = model(xb).squeeze()\n",
                "        loss_global = bce_element(logits_global, yb)\n",
                "\n",
                "        # --- Local RF-weighted loss ---\n",
                "        if tau < 1.0 and len(local_weights) > 0:\n",
                "            # Sample a subset of pairs for efficiency\n",
                "            n_pairs = min(len(local_weights), batch_size * 10)\n",
                "            pair_perm = torch.randperm(len(local_weights))[:n_pairs]\n",
                "\n",
                "            pi = pairs_i_t[pair_perm]\n",
                "            pj = pairs_j_t[pair_perm]\n",
                "            pw = weights_t[pair_perm]\n",
                "\n",
                "            # f(X_i) predicting Y_j, weighted by w(X_i, X_j)\n",
                "            logits_i = model(X_tr[pi]).squeeze()\n",
                "            targets_j = y_tr[pj]\n",
                "\n",
                "            # Weighted BCE: sum of w_ij * BCE(f(x_i), y_j)\n",
                "            per_pair_loss = nn.functional.binary_cross_entropy_with_logits(\n",
                "                logits_i, targets_j, reduction='none'\n",
                "            )\n",
                "            loss_local = (per_pair_loss * pw).mean()\n",
                "\n",
                "            # Combined loss\n",
                "            loss = tau * loss_global + (1 - tau) * loss_local\n",
                "        else:\n",
                "            loss = loss_global\n",
                "\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        # Validation\n",
                "        model.eval()\n",
                "        with torch.no_grad():\n",
                "            val_logits = model(X_va).squeeze()\n",
                "            val_loss = bce_element(val_logits, y_va).item()\n",
                "\n",
                "        if val_loss < best_val_loss - 1e-5:\n",
                "            best_val_loss = val_loss\n",
                "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
                "            wait = 0\n",
                "        else:\n",
                "            wait += 1\n",
                "            if wait >= patience:\n",
                "                break\n",
                "\n",
                "    model.load_state_dict(best_state)\n",
                "    model.eval()\n",
                "    return model, best_val_loss\n",
                "\n",
                "# --- τ Selection via CV on first target ---\n",
                "print(\"\\nSelecting τ via 3-fold CV on 7-day target...\")\n",
                "first_target = 'adopted_within_07_days'\n",
                "y_tau = df[first_target].values\n",
                "skf_tau = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
                "\n",
                "# First, train RF on full data to get kernel weights\n",
                "print(\"  Training RF for kernel weights...\")\n",
                "te_full = TargetEncoder(cols=CAT_COLS, smoothing=0.3)\n",
                "X_cat_enc_full = te_full.fit_transform(X_cat, y_tau)\n",
                "X_full = np.hstack([X_num.values, X_cat_enc_full.values, X_tfidf.values])\n",
                "\n",
                "rf_for_kernel = RandomForestClassifier(\n",
                "    n_estimators=100, max_features='sqrt', min_samples_split=5,\n",
                "    class_weight='balanced_subsample', random_state=42, n_jobs=-1\n",
                ")\n",
                "rf_for_kernel.fit(X_full, y_tau)\n",
                "print(\"  Computing RF kernel weights...\")\n",
                "rf_weights_full = compute_rf_kernel_weights(rf_for_kernel, X_full)\n",
                "\n",
                "n_samples = len(df)\n",
                "tau_candidates = [1/(4*n_samples), 1/(2*n_samples), 1/n_samples, 2/n_samples, 4/n_samples]\n",
                "\n",
                "best_tau = tau_candidates[2]  # Default: 1/n\n",
                "best_tau_score = float('inf')\n",
                "\n",
                "for tau_cand in tau_candidates:\n",
                "    tau_scores = []\n",
                "    for fold, (tr_idx, va_idx) in enumerate(skf_tau.split(np.zeros(len(df)), y_tau)):\n",
                "        te_tmp = TargetEncoder(cols=CAT_COLS, smoothing=0.3)\n",
                "        X_tr_cat = te_tmp.fit_transform(X_cat.iloc[tr_idx], y_tau[tr_idx])\n",
                "        X_va_cat = te_tmp.transform(X_cat.iloc[va_idx])\n",
                "\n",
                "        X_tr = np.hstack([X_num.iloc[tr_idx].values, X_tr_cat.values, X_tfidf.iloc[tr_idx].values])\n",
                "        X_va = np.hstack([X_num.iloc[va_idx].values, X_va_cat.values, X_tfidf.iloc[va_idx].values])\n",
                "\n",
                "        smote_tmp = SMOTE(random_state=42, sampling_strategy=0.3)\n",
                "        try:\n",
                "            X_tr_sm, y_tr_sm = smote_tmp.fit_resample(X_tr, y_tau[tr_idx])\n",
                "        except ValueError:\n",
                "            X_tr_sm, y_tr_sm = X_tr, y_tau[tr_idx]\n",
                "\n",
                "        model_tmp, _ = train_rwn_model(\n",
                "            X_tr_sm, y_tr_sm, X_va, y_tau[va_idx],\n",
                "            input_dim=X_tr_sm.shape[1], rf_weights=rf_weights_full,\n",
                "            train_indices=tr_idx, tau=tau_cand, h=256,\n",
                "            max_epochs=500, patience=20, dropout=0.3\n",
                "        )\n",
                "        with torch.no_grad():\n",
                "            logits = model_tmp(torch.FloatTensor(X_va).to(device)).squeeze()\n",
                "            probs = torch.sigmoid(logits).cpu().numpy()\n",
                "        tau_scores.append(log_loss(y_tau[va_idx], probs))\n",
                "\n",
                "    mean_ll = np.mean(tau_scores)\n",
                "    print(f\"  τ={tau_cand:.6f}: LogLoss={mean_ll:.4f}\")\n",
                "    if mean_ll < best_tau_score:\n",
                "        best_tau_score = mean_ll\n",
                "        best_tau = tau_cand\n",
                "\n",
                "print(f\"  → Best τ = {best_tau:.6f} (LogLoss={best_tau_score:.4f})\")\n",
                "\n",
                "# --- Full 5-Fold OOF Training with best τ ---\n",
                "print(f\"\\nTraining RWN with τ={best_tau:.6f}...\")\n",
                "\n",
                "rwn_results = {}\n",
                "rwn_models = {}\n",
                "rwn_target_encoders = {}\n",
                "rwn_oof_predictions = {}\n",
                "rwn_chained_probs = pd.DataFrame(index=df.index)\n",
                "\n",
                "for period, target in target_mapping.items():\n",
                "    print(f\"\\n{'='*30} {period} {'='*30}\")\n",
                "    y = df[target].values\n",
                "    chain_cols = [c for c in rwn_chained_probs.columns]\n",
                "\n",
                "    # Train RF for this target's kernel weights\n",
                "    te_rf = TargetEncoder(cols=CAT_COLS, smoothing=0.3)\n",
                "    X_cat_enc_rf = te_rf.fit_transform(X_cat, y)\n",
                "    X_rf = np.hstack([X_num.values, X_cat_enc_rf.values, X_tfidf.values])\n",
                "\n",
                "    rf_kernel = RandomForestClassifier(\n",
                "        n_estimators=100, max_features='sqrt', min_samples_split=5,\n",
                "        class_weight='balanced_subsample', random_state=42, n_jobs=-1\n",
                "    )\n",
                "    rf_kernel.fit(X_rf, y)\n",
                "    rf_weights = compute_rf_kernel_weights(rf_kernel, X_rf)\n",
                "\n",
                "    oof_probs = np.zeros(len(df))\n",
                "    fold_models_list = []\n",
                "    fold_te_list = []\n",
                "\n",
                "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(df)), y)):\n",
                "        te = TargetEncoder(cols=CAT_COLS, smoothing=0.3)\n",
                "        X_train_cat_enc = te.fit_transform(X_cat.iloc[train_idx], y[train_idx])\n",
                "        X_val_cat_enc = te.transform(X_cat.iloc[val_idx])\n",
                "\n",
                "        parts_train = [X_num.iloc[train_idx].values, X_train_cat_enc.values, X_tfidf.iloc[train_idx].values]\n",
                "        parts_val = [X_num.iloc[val_idx].values, X_val_cat_enc.values, X_tfidf.iloc[val_idx].values]\n",
                "        if len(chain_cols) > 0:\n",
                "            parts_train.append(rwn_chained_probs[chain_cols].iloc[train_idx].values)\n",
                "            parts_val.append(rwn_chained_probs[chain_cols].iloc[val_idx].values)\n",
                "\n",
                "        X_train = np.hstack(parts_train)\n",
                "        X_val = np.hstack(parts_val)\n",
                "\n",
                "        smote = SMOTE(random_state=42, sampling_strategy=0.3)\n",
                "        try:\n",
                "            X_train_sm, y_train_sm = smote.fit_resample(X_train, y[train_idx])\n",
                "        except ValueError:\n",
                "            X_train_sm, y_train_sm = X_train, y[train_idx]\n",
                "\n",
                "        input_dim = X_train_sm.shape[1]\n",
                "        model, val_loss = train_rwn_model(\n",
                "            X_train_sm, y_train_sm, X_val, y[val_idx],\n",
                "            input_dim=input_dim, rf_weights=rf_weights,\n",
                "            train_indices=train_idx, tau=best_tau, h=512,\n",
                "            max_epochs=2000, patience=50, dropout=0.3\n",
                "        )\n",
                "\n",
                "        with torch.no_grad():\n",
                "            X_va_t = torch.FloatTensor(X_val).to(device)\n",
                "            logits = model(X_va_t).squeeze()\n",
                "            fold_probs = torch.sigmoid(logits).cpu().numpy()\n",
                "\n",
                "        oof_probs[val_idx] = fold_probs\n",
                "        fold_models_list.append(model)\n",
                "        fold_te_list.append(te)\n",
                "\n",
                "        fold_auc = roc_auc_score(y[val_idx], fold_probs)\n",
                "        print(f\"  Fold {fold+1}: AUC={fold_auc:.4f}, val_loss={val_loss:.4f}\")\n",
                "\n",
                "    oof_auc = roc_auc_score(y, oof_probs)\n",
                "    oof_ll = log_loss(y, oof_probs)\n",
                "    print(f\"  ✓ OOF AUC={oof_auc:.4f}, LogLoss={oof_ll:.4f}\")\n",
                "\n",
                "    rwn_chained_probs[target] = oof_probs\n",
                "    rwn_results[period] = {'auc': oof_auc, 'logloss': oof_ll}\n",
                "    rwn_models[period] = fold_models_list\n",
                "    rwn_target_encoders[period] = fold_te_list\n",
                "    rwn_oof_predictions[period] = oof_probs\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"RWN SUMMARY:\")\n",
                "for period, res in rwn_results.items():\n",
                "    print(f\"  {period}: AUC={res['auc']:.4f}, LogLoss={res['logloss']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "md_comparison",
            "metadata": {},
            "source": [
                "## Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_comparison",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Model Comparison\n",
                "# ============================================================\n",
                "print(\"=\" * 70)\n",
                "print(\"MODEL COMPARISON\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(f\"\\n{'Model':<8} {'Period':<10} {'AUC':>8} {'LogLoss':>10}\")\n",
                "print(\"-\" * 38)\n",
                "for period in ['7 Days', '90 Days', '120 Days']:\n",
                "    for name, results in [('RF', rf_results), ('NN', nn_results), ('RWN', rwn_results)]:\n",
                "        r = results[period]\n",
                "        print(f\"{name:<8} {period:<10} {r['auc']:>8.4f} {r['logloss']:>10.4f}\")\n",
                "    print()\n",
                "\n",
                "print(\"\\nAverage LogLoss:\")\n",
                "for name, results in [('RF', rf_results), ('NN', nn_results), ('RWN', rwn_results)]:\n",
                "    avg_ll = np.mean([r['logloss'] for r in results.values()])\n",
                "    avg_auc = np.mean([r['auc'] for r in results.values()])\n",
                "    print(f\"  {name}: AUC={avg_auc:.4f}, LogLoss={avg_ll:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "md_inference",
            "metadata": {},
            "source": [
                "## Test Inference & Submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell_inference",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Test Inference & Submission Generation\n",
                "# ============================================================\n",
                "\n",
                "# --- Load test data ---\n",
                "test_df = pd.read_csv(\"Original Data/Test.csv\")\n",
                "print(f\"Test data: {test_df.shape}\")\n",
                "\n",
                "# --- Topic preprocessing ---\n",
                "test_df['clean_topics'] = test_df['topics_list'].apply(clean_and_flat_topics)\n",
                "topics_encoded_test = mlb.transform(test_df['clean_topics'])\n",
                "topic_columns_list = [f'topic_{t}' for t in mlb.classes_]\n",
                "topics_df_test = pd.DataFrame(topics_encoded_test, columns=topic_columns_list, index=test_df.index)\n",
                "test_df = pd.concat([test_df, topics_df_test], axis=1)\n",
                "\n",
                "# Topic text\n",
                "test_df['topic_text'] = test_df[topic_columns].apply(topics_to_text, axis=1)\n",
                "\n",
                "# Date features\n",
                "test_df['training_day'] = pd.to_datetime(test_df['training_day'], dayfirst=True)\n",
                "test_df['training_year'] = test_df['training_day'].dt.year\n",
                "test_df['training_month'] = test_df['training_day'].dt.month\n",
                "test_df['training_day_number'] = test_df['training_day'].dt.day\n",
                "test_df['training_dayofweek'] = test_df['training_day'].dt.dayofweek\n",
                "\n",
                "# Numeric and categorical test features\n",
                "X_test_num = test_df[NUM_COLS].copy()\n",
                "X_test_cat = test_df[CAT_COLS].copy().astype(str).fillna(\"NA\")\n",
                "\n",
                "# TF-IDF\n",
                "X_test_tfidf = tfidf.transform(test_df['topic_text']).toarray()\n",
                "\n",
                "submission_mapping = {\n",
                "    \"7 Days\": [\"Target_07_AUC\", \"Target_07_LogLoss\"],\n",
                "    \"90 Days\": [\"Target_90_AUC\", \"Target_90_LogLoss\"],\n",
                "    \"120 Days\": [\"Target_120_AUC\", \"Target_120_LogLoss\"]\n",
                "}\n",
                "\n",
                "target_order = [\n",
                "    (\"7 Days\", \"adopted_within_07_days\"),\n",
                "    (\"90 Days\", \"adopted_within_90_days\"),\n",
                "    (\"120 Days\", \"adopted_within_120_days\")\n",
                "]\n",
                "\n",
                "# --- Helper to generate predictions for sklearn models ---\n",
                "def generate_submission_sklearn(model_name, models_dict, te_dict):\n",
                "    \"\"\"Generate chained submission from sklearn fold models.\"\"\"\n",
                "    submission = pd.DataFrame()\n",
                "    submission[\"ID\"] = test_df[\"ID\"]\n",
                "    chained_test_probs = pd.DataFrame(index=test_df.index)\n",
                "\n",
                "    for period, target in target_order:\n",
                "        fold_models = models_dict[period]\n",
                "        fold_tes = te_dict[period]\n",
                "        chain_cols = [c for c in chained_test_probs.columns]\n",
                "        fold_predictions = []\n",
                "\n",
                "        for fold_model, fold_te in zip(fold_models, fold_tes):\n",
                "            X_test_cat_enc = fold_te.transform(X_test_cat)\n",
                "            parts = [X_test_num.values, X_test_cat_enc.values, X_test_tfidf]\n",
                "            if len(chain_cols) > 0:\n",
                "                parts.append(chained_test_probs[chain_cols].values)\n",
                "            X_test_final = np.hstack(parts)\n",
                "            probs = fold_model.predict_proba(X_test_final)[:, 1]\n",
                "            fold_predictions.append(probs)\n",
                "\n",
                "        avg_probs = np.mean(fold_predictions, axis=0)\n",
                "        for col in submission_mapping[period]:\n",
                "            submission[col] = avg_probs\n",
                "        chained_test_probs[target] = avg_probs\n",
                "        print(f\"  {model_name} {period}: min={avg_probs.min():.4f}, max={avg_probs.max():.4f}, mean={avg_probs.mean():.4f}\")\n",
                "\n",
                "    return submission\n",
                "\n",
                "# --- Helper to generate predictions for PyTorch models ---\n",
                "def generate_submission_torch(model_name, models_dict, te_dict):\n",
                "    \"\"\"Generate chained submission from PyTorch fold models.\"\"\"\n",
                "    submission = pd.DataFrame()\n",
                "    submission[\"ID\"] = test_df[\"ID\"]\n",
                "    chained_test_probs = pd.DataFrame(index=test_df.index)\n",
                "\n",
                "    for period, target in target_order:\n",
                "        fold_models = models_dict[period]\n",
                "        fold_tes = te_dict[period]\n",
                "        chain_cols = [c for c in chained_test_probs.columns]\n",
                "        fold_predictions = []\n",
                "\n",
                "        for fold_model, fold_te in zip(fold_models, fold_tes):\n",
                "            X_test_cat_enc = fold_te.transform(X_test_cat)\n",
                "            parts = [X_test_num.values, X_test_cat_enc.values, X_test_tfidf]\n",
                "            if len(chain_cols) > 0:\n",
                "                parts.append(chained_test_probs[chain_cols].values)\n",
                "            X_test_final = np.hstack(parts)\n",
                "\n",
                "            fold_model.eval()\n",
                "            with torch.no_grad():\n",
                "                X_t = torch.FloatTensor(X_test_final).to(device)\n",
                "                logits = fold_model(X_t).squeeze()\n",
                "                probs = torch.sigmoid(logits).cpu().numpy()\n",
                "            fold_predictions.append(probs)\n",
                "\n",
                "        avg_probs = np.mean(fold_predictions, axis=0)\n",
                "        for col in submission_mapping[period]:\n",
                "            submission[col] = avg_probs\n",
                "        chained_test_probs[target] = avg_probs\n",
                "        print(f\"  {model_name} {period}: min={avg_probs.min():.4f}, max={avg_probs.max():.4f}, mean={avg_probs.mean():.4f}\")\n",
                "\n",
                "    return submission\n",
                "\n",
                "# --- Generate all submissions ---\n",
                "print(\"\\nGenerating RF submission...\")\n",
                "sub_rf = generate_submission_sklearn(\"RF\", rf_models, rf_target_encoders)\n",
                "sub_rf.to_csv(\"submission_rf.csv\", index=False)\n",
                "print(\"  Saved: submission_rf.csv\")\n",
                "\n",
                "print(\"\\nGenerating NN submission...\")\n",
                "sub_nn = generate_submission_torch(\"NN\", nn_models, nn_target_encoders)\n",
                "sub_nn.to_csv(\"submission_nn.csv\", index=False)\n",
                "print(\"  Saved: submission_nn.csv\")\n",
                "\n",
                "print(\"\\nGenerating RWN submission...\")\n",
                "sub_rwn = generate_submission_torch(\"RWN\", rwn_models, rwn_target_encoders)\n",
                "sub_rwn.to_csv(\"submission_rwn.csv\", index=False)\n",
                "print(\"  Saved: submission_rwn.csv\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"All submissions generated!\")\n",
                "print(\"=\"*70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (DigiCow)",
            "language": "python",
            "name": "digicow-venv"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
